{"id":"osrs-flips-4cm","title":"Implement historical backfill from /timeseries","description":"Backfill historical data using the /timeseries endpoint.\n\n## Requirements\n- Fetch 24h → 1h → 5m buckets per item\n- Work backwards from present\n- Rate-limited to respect API\n- Populate price_buckets with source='api'\n\n## Considerations\n- Need to handle 4000+ items\n- Should be resumable if interrupted\n- Track backfill progress\n\n## Acceptance Criteria\n- Can backfill specified time range\n- Respects rate limits\n- Progress tracking/resumption","status":"open","priority":2,"issue_type":"task","owner":"steven@fitz.gg","created_at":"2026-02-03T08:57:40.188909961Z","created_by":"Steven Fitzpatrick","updated_at":"2026-02-03T08:57:40.188909961Z","dependencies":[{"issue_id":"osrs-flips-4cm","depends_on_id":"osrs-flips-al8","type":"parent-child","created_at":"2026-02-03T08:58:12.229133335Z","created_by":"Steven Fitzpatrick"},{"issue_id":"osrs-flips-4cm","depends_on_id":"osrs-flips-bl0","type":"blocks","created_at":"2026-02-03T08:58:35.737026375Z","created_by":"Steven Fitzpatrick"}]}
{"id":"osrs-flips-4xj","title":"Implement gap filling service","description":"Fill missing data gaps using /5m endpoint and computed buckets.\n\n## Requirements\n- Hourly: Check for missing 5m buckets in last 24h\n- Fill gaps via /5m API endpoint\n- Daily: Backfill missing larger buckets\n- Compute own buckets from raw data for comparison\n\n## Design Notes\n- Use /5m for surgical gap filling (point-in-time snapshot)\n- Prefer API buckets over computed (include volume data)\n- Validate computed vs API buckets\n\n## Acceptance Criteria\n- Automated gap detection\n- Gap filling via API\n- Computed bucket generation for validation","status":"open","priority":3,"issue_type":"task","owner":"steven@fitz.gg","created_at":"2026-02-03T08:57:45.990695519Z","created_by":"Steven Fitzpatrick","updated_at":"2026-02-03T08:57:45.990695519Z","dependencies":[{"issue_id":"osrs-flips-4xj","depends_on_id":"osrs-flips-al8","type":"parent-child","created_at":"2026-02-03T08:58:12.254113803Z","created_by":"Steven Fitzpatrick"},{"issue_id":"osrs-flips-4xj","depends_on_id":"osrs-flips-e5y","type":"blocks","created_at":"2026-02-03T08:58:35.764171619Z","created_by":"Steven Fitzpatrick"}]}
{"id":"osrs-flips-6yr","title":"Create item metadata table and population strategy","description":"Store item metadata (name, examine text, buy limits, etc.) for enriching price data.\n\n## Context\nPrice tables reference item_id (INTEGER), but we need item details for:\n- Display names in alerts/reports\n- Buy limits for flip calculations\n- Item categories/tags for filtering\n\n## API Source\nOSRS Wiki provides item mapping endpoint - need to investigate:\n- Endpoint URL and format\n- Update frequency\n- Available fields\n\n## Schema (tentative)\n```sql\nCREATE TABLE items (\n    item_id     INTEGER PRIMARY KEY,\n    name        TEXT NOT NULL,\n    examine     TEXT,\n    members     BOOLEAN,\n    buy_limit   INTEGER,\n    high_alch   INTEGER,\n    low_alch    INTEGER,\n    -- Consider: icon_url, wiki_url, category, etc.\n    updated_at  TIMESTAMPTZ DEFAULT NOW()\n);\n```\n\n## Open Questions\n- [ ] How often does item data change? (new items, limit changes)\n- [ ] Populate on startup, or separate sync job?\n- [ ] Should this block other work, or can we enrich later?\n\n## Acceptance Criteria\n- Schema designed based on available API data\n- Population strategy defined\n- Migration created","status":"open","priority":3,"issue_type":"task","owner":"steven@fitz.gg","created_at":"2026-02-03T09:14:27.003308716Z","created_by":"Steven Fitzpatrick","updated_at":"2026-02-03T09:14:27.003308716Z","dependencies":[{"issue_id":"osrs-flips-6yr","depends_on_id":"osrs-flips-al8","type":"parent-child","created_at":"2026-02-03T09:17:11.229152851Z","created_by":"Steven Fitzpatrick"},{"issue_id":"osrs-flips-6yr","depends_on_id":"osrs-flips-bl0","type":"blocks","created_at":"2026-02-03T09:17:11.257095096Z","created_by":"Steven Fitzpatrick"}]}
{"id":"osrs-flips-al8","title":"Persistent Price Data Storage","description":"Implement PostgreSQL + TimescaleDB storage for price data to reduce API abuse and enable historical analysis.\n\n## Goals\n- Store price observations from /latest polling (real-time)\n- Store pre-aggregated bucket data from API (historical)\n- Enable gap filling and data validation\n- Support hybrid queries across raw and bucketed data\n\n## Architecture Decision\n**Separate binary: `cmd/collector/main.go`**\n- Headless service dedicated to database maintenance\n- Runs independently from the Discord bot (`cmd/bot/`)\n- Responsibilities: polling, backfill, gap filling, lifecycle management\n- Query interface / Discord bot integration deferred to future effort\n\n## Configuration\n- Database connection via `DATABASE_URL` environment variable\n- External PostgreSQL + TimescaleDB instance (not managed by this project)\n\n## Reference\nSee plans/Persistance.md for detailed design notes.","status":"open","priority":1,"issue_type":"epic","owner":"steven@fitz.gg","created_at":"2026-02-03T08:57:20.555262025Z","created_by":"Steven Fitzpatrick","updated_at":"2026-02-03T09:28:45.359791309Z"}
{"id":"osrs-flips-bl0","title":"Create database schema for price data","description":"Implement the price data schema using TimescaleDB hypertables.\n\n## Design Decisions\n- Use TimescaleDB hypertables (learning exercise, simple automatic chunking)\n- Prices stored as INTEGER (matches in-game gold pieces, PostgreSQL aggregates return NUMERIC for precision)\n- Item IDs are INTEGER\n- Include ingested_at timestamps for debugging/auditing\n\n## Tables\n\n### 1. `price_observations` - Raw data from /latest polling\n```sql\nCREATE TABLE price_observations (\n    item_id        INTEGER NOT NULL,\n    observed_at    TIMESTAMPTZ NOT NULL,\n    high_price     INTEGER,\n    high_time      TIMESTAMPTZ,\n    low_price      INTEGER,\n    low_time       TIMESTAMPTZ,\n    ingested_at    TIMESTAMPTZ DEFAULT NOW(),\n    PRIMARY KEY (item_id, observed_at)\n);\n-- Convert to hypertable (chunks by observed_at)\nSELECT create_hypertable('price_observations', 'observed_at');\n```\n\n### 2. `price_buckets` - Pre-aggregated bucket data from API\n```sql\nCREATE TABLE price_buckets (\n    item_id           INTEGER NOT NULL,\n    bucket_start      TIMESTAMPTZ NOT NULL,\n    bucket_size       TEXT NOT NULL,  -- '5m', '1h', '24h'\n    avg_high_price    INTEGER,\n    high_price_volume BIGINT,\n    avg_low_price     INTEGER,\n    low_price_volume  BIGINT,\n    source            TEXT NOT NULL DEFAULT 'api',  -- 'api' or 'computed'\n    ingested_at       TIMESTAMPTZ DEFAULT NOW(),\n    PRIMARY KEY (item_id, bucket_start, bucket_size)\n);\nSELECT create_hypertable('price_buckets', 'bucket_start');\n```\n\n## Indexes (for target queries)\n\nTarget use cases:\n- Technical analysis (MACD, RSI): time-series for specific items\n- Price threshold alerts: lookup by item, compare current price\n\n```sql\n-- Hypertables auto-create time index, add item lookups\nCREATE INDEX idx_observations_item ON price_observations (item_id, observed_at DESC);\nCREATE INDEX idx_buckets_item ON price_buckets (item_id, bucket_start DESC);\n-- For threshold alerting (recent prices by item)\nCREATE INDEX idx_observations_item_prices ON price_observations (item_id, high_price, low_price);\n```\n\n## Tasks\n- [ ] Create migration: 001_enable_timescaledb.up.sql (CREATE EXTENSION)\n- [ ] Create migration: 002_price_observations.up.sql\n- [ ] Create migration: 003_price_buckets.up.sql\n- [ ] Create corresponding .down.sql migrations\n- [ ] Test migrations against local TimescaleDB instance\n- [ ] Document chunk interval tuning (default vs custom)\n\n## Acceptance Criteria\n- Hypertables created successfully\n- Indexes support item lookup + time range queries\n- Migrations reversible\n- Schema matches API response structure for easy ingestion\n\n## Notes\n- Item metadata table tracked separately (needs population strategy)\n- Chunk interval: start with TimescaleDB defaults, tune later based on data volume","status":"closed","priority":2,"issue_type":"task","owner":"steven@fitz.gg","created_at":"2026-02-03T08:57:30.28724963Z","created_by":"Steven Fitzpatrick","updated_at":"2026-02-03T10:12:54.550212955Z","closed_at":"2026-02-03T10:12:54.550212955Z","close_reason":"Closed","dependencies":[{"issue_id":"osrs-flips-bl0","depends_on_id":"osrs-flips-al8","type":"parent-child","created_at":"2026-02-03T08:58:12.175698056Z","created_by":"Steven Fitzpatrick"},{"issue_id":"osrs-flips-bl0","depends_on_id":"osrs-flips-vzg","type":"blocks","created_at":"2026-02-03T08:58:35.678932742Z","created_by":"Steven Fitzpatrick"}]}
{"id":"osrs-flips-e5y","title":"Implement /latest polling ingestion","description":"Real-time continuous polling of the /latest endpoint.\n\n## Requirements\n- Poll /latest every 30-60 seconds\n- Insert all ~4000 items into price_observations\n- Handle connection failures gracefully\n- Configurable polling interval\n\n## Design Notes\n- Single API call captures all items\n- Enables sub-bucket resolution\n- Foundation for immediate alerting\n\n## Acceptance Criteria\n- Polling service runs continuously\n- Data written to price_observations table\n- Graceful error handling and retry logic","status":"open","priority":2,"issue_type":"task","owner":"steven@fitz.gg","created_at":"2026-02-03T08:57:34.748869102Z","created_by":"Steven Fitzpatrick","updated_at":"2026-02-03T08:57:34.748869102Z","dependencies":[{"issue_id":"osrs-flips-e5y","depends_on_id":"osrs-flips-al8","type":"parent-child","created_at":"2026-02-03T08:58:12.203366256Z","created_by":"Steven Fitzpatrick"},{"issue_id":"osrs-flips-e5y","depends_on_id":"osrs-flips-bl0","type":"blocks","created_at":"2026-02-03T08:58:35.709461426Z","created_by":"Steven Fitzpatrick"}]}
{"id":"osrs-flips-ft5","title":"Update query layer to use persistent storage","description":"Refactor existing query/analysis code to use database instead of direct API calls.\n\n## Requirements\n- Recent alerts/analysis: Query price_observations\n- Historical trends: Query price_buckets with bucket_size\n- Hybrid queries: Union recent raw with historical buckets\n- Fallback to API if data missing\n\n## Acceptance Criteria\n- Existing functionality preserved\n- Queries use local data first\n- Reduced API dependency","status":"open","priority":2,"issue_type":"task","owner":"steven@fitz.gg","created_at":"2026-02-03T08:57:54.537487413Z","created_by":"Steven Fitzpatrick","updated_at":"2026-02-03T08:57:54.537487413Z","dependencies":[{"issue_id":"osrs-flips-ft5","depends_on_id":"osrs-flips-al8","type":"parent-child","created_at":"2026-02-03T08:58:12.305038029Z","created_by":"Steven Fitzpatrick"},{"issue_id":"osrs-flips-ft5","depends_on_id":"osrs-flips-bl0","type":"blocks","created_at":"2026-02-03T08:58:35.815719373Z","created_by":"Steven Fitzpatrick"}]}
{"id":"osrs-flips-i52","title":"Implement data lifecycle management","description":"Manage retention and pruning of price data.\n\n## Requirements\n- Raw observations: Keep 7-30 days (configurable)\n- Bucket aggregates: Keep indefinitely\n- Prune old raw observations daily\n- Drop old partitions efficiently\n\n## Acceptance Criteria\n- Configurable retention periods\n- Automated pruning job\n- Partition management","status":"open","priority":3,"issue_type":"task","owner":"steven@fitz.gg","created_at":"2026-02-03T08:57:49.144224394Z","created_by":"Steven Fitzpatrick","updated_at":"2026-02-03T08:57:49.144224394Z","dependencies":[{"issue_id":"osrs-flips-i52","depends_on_id":"osrs-flips-al8","type":"parent-child","created_at":"2026-02-03T08:58:12.280741379Z","created_by":"Steven Fitzpatrick"},{"issue_id":"osrs-flips-i52","depends_on_id":"osrs-flips-bl0","type":"blocks","created_at":"2026-02-03T08:58:35.790290074Z","created_by":"Steven Fitzpatrick"}]}
{"id":"osrs-flips-vzg","title":"Set up PostgreSQL + TimescaleDB infrastructure","description":"Configure database connectivity and migration tooling for price data storage.\n\n## Decisions\n- Database: External PostgreSQL + TimescaleDB instance (not managed in docker-compose)\n- Driver: pgx (native Go, built-in connection pooling)\n- Migrations: golang-migrate/migrate\n\n## Tasks\n- [ ] Add pgx dependency (`github.com/jackc/pgx/v5`)\n- [ ] Add golang-migrate dependency (`github.com/golang-migrate/migrate/v4`)\n- [ ] Create database configuration struct\n- [ ] Add environment variable support for DB connection:\n  - `DATABASE_URL` (connection string) or individual vars:\n  - `DB_HOST`, `DB_PORT`, `DB_USER`, `DB_PASSWORD`, `DB_NAME`\n  - `DB_POOL_MAX_CONNS` (optional, default sensible)\n  - `DB_SSL_MODE` (optional)\n- [ ] Create connection pool initialization with pgxpool\n- [ ] Set up migrations directory structure (`migrations/`)\n- [ ] Add migration runner to application startup\n- [ ] Update docker-compose.yml with new env vars\n- [ ] Document database setup in README\n\n## Acceptance Criteria\n- Application connects to external PostgreSQL via environment config\n- pgxpool connection pool configured with sensible defaults\n- Migration framework runs on startup (or via flag)\n- Graceful handling of connection failures","status":"closed","priority":2,"issue_type":"task","owner":"steven@fitz.gg","created_at":"2026-02-03T08:57:25.156537925Z","created_by":"Steven Fitzpatrick","updated_at":"2026-02-03T10:09:54.182431381Z","closed_at":"2026-02-03T10:09:54.182431381Z","close_reason":"Closed","dependencies":[{"issue_id":"osrs-flips-vzg","depends_on_id":"osrs-flips-al8","type":"parent-child","created_at":"2026-02-03T08:58:08.394276765Z","created_by":"Steven Fitzpatrick"}]}
